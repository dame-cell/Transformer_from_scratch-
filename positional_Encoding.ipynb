{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c415915-4ac2-4007-ad24-55ee0174401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e7912b-4429-4833-938c-4ef36f0319b9",
   "metadata": {},
   "source": [
    "# ABSOLUTE POSITIONAL ENCODING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc967120-c98a-4876-95c8-c7c954d1bcb0",
   "metadata": {},
   "source": [
    "## In absolute positional encoding each tokens get its own positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b727f2d7-b72b-4a6a-bb01-1dcaed943c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self):\n",
    "        even_i = torch.arange(0, self.d_model, 2).float()\n",
    "        print(\"even_i:\",even_i)\n",
    "        denominator = torch.pow(10000, even_i/self.d_model)\n",
    "        print(\"denominator:\",denominator)\n",
    "        position = (torch.arange(self.max_sequence_length).reshape(self.max_sequence_length, 1))\n",
    "        print(\"position:\",position)\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        print(\"even_PE:\",even_PE)\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        print(\"odd_PE:\",odd_PE)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "        print(\"stacked:\",stacked)\n",
    "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        return PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "851ff33b-c161-49cf-9236-0ff5b7cd30d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "even_i: tensor([  0.,   2.,   4.,   6.,   8.,  10.,  12.,  14.,  16.,  18.,  20.,  22.,\n",
      "         24.,  26.,  28.,  30.,  32.,  34.,  36.,  38.,  40.,  42.,  44.,  46.,\n",
      "         48.,  50.,  52.,  54.,  56.,  58.,  60.,  62.,  64.,  66.,  68.,  70.,\n",
      "         72.,  74.,  76.,  78.,  80.,  82.,  84.,  86.,  88.,  90.,  92.,  94.,\n",
      "         96.,  98., 100., 102., 104., 106., 108., 110., 112., 114., 116., 118.,\n",
      "        120., 122., 124., 126., 128., 130., 132., 134., 136., 138., 140., 142.,\n",
      "        144., 146., 148., 150., 152., 154., 156., 158., 160., 162., 164., 166.,\n",
      "        168., 170., 172., 174., 176., 178., 180., 182., 184., 186., 188., 190.,\n",
      "        192., 194., 196., 198., 200., 202., 204., 206., 208., 210., 212., 214.,\n",
      "        216., 218., 220., 222., 224., 226., 228., 230., 232., 234., 236., 238.,\n",
      "        240., 242., 244., 246., 248., 250., 252., 254., 256., 258., 260., 262.,\n",
      "        264., 266., 268., 270., 272., 274., 276., 278., 280., 282., 284., 286.,\n",
      "        288., 290., 292., 294., 296., 298., 300., 302., 304., 306., 308., 310.,\n",
      "        312., 314., 316., 318., 320., 322., 324., 326., 328., 330., 332., 334.,\n",
      "        336., 338., 340., 342., 344., 346., 348., 350., 352., 354., 356., 358.,\n",
      "        360., 362., 364., 366., 368., 370., 372., 374., 376., 378., 380., 382.,\n",
      "        384., 386., 388., 390., 392., 394., 396., 398., 400., 402., 404., 406.,\n",
      "        408., 410., 412., 414., 416., 418., 420., 422., 424., 426., 428., 430.,\n",
      "        432., 434., 436., 438., 440., 442., 444., 446., 448., 450., 452., 454.,\n",
      "        456., 458., 460., 462., 464., 466., 468., 470., 472., 474., 476., 478.,\n",
      "        480., 482., 484., 486., 488., 490., 492., 494., 496., 498., 500., 502.,\n",
      "        504., 506., 508., 510.])\n",
      "denominator: tensor([1.0000e+00, 1.0366e+00, 1.0746e+00, 1.1140e+00, 1.1548e+00, 1.1971e+00,\n",
      "        1.2409e+00, 1.2864e+00, 1.3335e+00, 1.3824e+00, 1.4330e+00, 1.4855e+00,\n",
      "        1.5399e+00, 1.5963e+00, 1.6548e+00, 1.7154e+00, 1.7783e+00, 1.8434e+00,\n",
      "        1.9110e+00, 1.9810e+00, 2.0535e+00, 2.1288e+00, 2.2067e+00, 2.2876e+00,\n",
      "        2.3714e+00, 2.4582e+00, 2.5483e+00, 2.6416e+00, 2.7384e+00, 2.8387e+00,\n",
      "        2.9427e+00, 3.0505e+00, 3.1623e+00, 3.2781e+00, 3.3982e+00, 3.5227e+00,\n",
      "        3.6517e+00, 3.7855e+00, 3.9242e+00, 4.0679e+00, 4.2170e+00, 4.3714e+00,\n",
      "        4.5316e+00, 4.6976e+00, 4.8697e+00, 5.0481e+00, 5.2330e+00, 5.4247e+00,\n",
      "        5.6234e+00, 5.8294e+00, 6.0430e+00, 6.2643e+00, 6.4938e+00, 6.7317e+00,\n",
      "        6.9783e+00, 7.2339e+00, 7.4989e+00, 7.7737e+00, 8.0584e+00, 8.3536e+00,\n",
      "        8.6596e+00, 8.9769e+00, 9.3057e+00, 9.6466e+00, 1.0000e+01, 1.0366e+01,\n",
      "        1.0746e+01, 1.1140e+01, 1.1548e+01, 1.1971e+01, 1.2409e+01, 1.2864e+01,\n",
      "        1.3335e+01, 1.3824e+01, 1.4330e+01, 1.4855e+01, 1.5399e+01, 1.5963e+01,\n",
      "        1.6548e+01, 1.7154e+01, 1.7783e+01, 1.8434e+01, 1.9110e+01, 1.9810e+01,\n",
      "        2.0535e+01, 2.1288e+01, 2.2067e+01, 2.2876e+01, 2.3714e+01, 2.4582e+01,\n",
      "        2.5483e+01, 2.6416e+01, 2.7384e+01, 2.8387e+01, 2.9427e+01, 3.0505e+01,\n",
      "        3.1623e+01, 3.2781e+01, 3.3982e+01, 3.5227e+01, 3.6517e+01, 3.7855e+01,\n",
      "        3.9242e+01, 4.0679e+01, 4.2170e+01, 4.3714e+01, 4.5316e+01, 4.6976e+01,\n",
      "        4.8697e+01, 5.0481e+01, 5.2330e+01, 5.4247e+01, 5.6234e+01, 5.8294e+01,\n",
      "        6.0430e+01, 6.2643e+01, 6.4938e+01, 6.7317e+01, 6.9783e+01, 7.2339e+01,\n",
      "        7.4989e+01, 7.7737e+01, 8.0584e+01, 8.3536e+01, 8.6596e+01, 8.9769e+01,\n",
      "        9.3057e+01, 9.6466e+01, 1.0000e+02, 1.0366e+02, 1.0746e+02, 1.1140e+02,\n",
      "        1.1548e+02, 1.1971e+02, 1.2409e+02, 1.2864e+02, 1.3335e+02, 1.3824e+02,\n",
      "        1.4330e+02, 1.4855e+02, 1.5399e+02, 1.5963e+02, 1.6548e+02, 1.7154e+02,\n",
      "        1.7783e+02, 1.8434e+02, 1.9110e+02, 1.9810e+02, 2.0535e+02, 2.1288e+02,\n",
      "        2.2067e+02, 2.2876e+02, 2.3714e+02, 2.4582e+02, 2.5483e+02, 2.6416e+02,\n",
      "        2.7384e+02, 2.8387e+02, 2.9427e+02, 3.0505e+02, 3.1623e+02, 3.2781e+02,\n",
      "        3.3982e+02, 3.5227e+02, 3.6517e+02, 3.7855e+02, 3.9242e+02, 4.0679e+02,\n",
      "        4.2170e+02, 4.3714e+02, 4.5316e+02, 4.6976e+02, 4.8697e+02, 5.0481e+02,\n",
      "        5.2330e+02, 5.4247e+02, 5.6234e+02, 5.8294e+02, 6.0430e+02, 6.2643e+02,\n",
      "        6.4938e+02, 6.7317e+02, 6.9783e+02, 7.2339e+02, 7.4989e+02, 7.7737e+02,\n",
      "        8.0584e+02, 8.3536e+02, 8.6596e+02, 8.9769e+02, 9.3057e+02, 9.6466e+02,\n",
      "        1.0000e+03, 1.0366e+03, 1.0746e+03, 1.1140e+03, 1.1548e+03, 1.1971e+03,\n",
      "        1.2409e+03, 1.2864e+03, 1.3335e+03, 1.3824e+03, 1.4330e+03, 1.4855e+03,\n",
      "        1.5399e+03, 1.5963e+03, 1.6548e+03, 1.7154e+03, 1.7783e+03, 1.8434e+03,\n",
      "        1.9110e+03, 1.9810e+03, 2.0535e+03, 2.1288e+03, 2.2067e+03, 2.2876e+03,\n",
      "        2.3714e+03, 2.4582e+03, 2.5483e+03, 2.6416e+03, 2.7384e+03, 2.8387e+03,\n",
      "        2.9427e+03, 3.0505e+03, 3.1623e+03, 3.2781e+03, 3.3982e+03, 3.5227e+03,\n",
      "        3.6517e+03, 3.7855e+03, 3.9242e+03, 4.0679e+03, 4.2170e+03, 4.3714e+03,\n",
      "        4.5316e+03, 4.6976e+03, 4.8697e+03, 5.0481e+03, 5.2330e+03, 5.4247e+03,\n",
      "        5.6234e+03, 5.8294e+03, 6.0430e+03, 6.2643e+03, 6.4938e+03, 6.7317e+03,\n",
      "        6.9783e+03, 7.2339e+03, 7.4989e+03, 7.7737e+03, 8.0584e+03, 8.3536e+03,\n",
      "        8.6596e+03, 8.9769e+03, 9.3057e+03, 9.6466e+03])\n",
      "position: tensor([[  0],\n",
      "        [  1],\n",
      "        [  2],\n",
      "        [  3],\n",
      "        [  4],\n",
      "        [  5],\n",
      "        [  6],\n",
      "        [  7],\n",
      "        [  8],\n",
      "        [  9],\n",
      "        [ 10],\n",
      "        [ 11],\n",
      "        [ 12],\n",
      "        [ 13],\n",
      "        [ 14],\n",
      "        [ 15],\n",
      "        [ 16],\n",
      "        [ 17],\n",
      "        [ 18],\n",
      "        [ 19],\n",
      "        [ 20],\n",
      "        [ 21],\n",
      "        [ 22],\n",
      "        [ 23],\n",
      "        [ 24],\n",
      "        [ 25],\n",
      "        [ 26],\n",
      "        [ 27],\n",
      "        [ 28],\n",
      "        [ 29],\n",
      "        [ 30],\n",
      "        [ 31],\n",
      "        [ 32],\n",
      "        [ 33],\n",
      "        [ 34],\n",
      "        [ 35],\n",
      "        [ 36],\n",
      "        [ 37],\n",
      "        [ 38],\n",
      "        [ 39],\n",
      "        [ 40],\n",
      "        [ 41],\n",
      "        [ 42],\n",
      "        [ 43],\n",
      "        [ 44],\n",
      "        [ 45],\n",
      "        [ 46],\n",
      "        [ 47],\n",
      "        [ 48],\n",
      "        [ 49],\n",
      "        [ 50],\n",
      "        [ 51],\n",
      "        [ 52],\n",
      "        [ 53],\n",
      "        [ 54],\n",
      "        [ 55],\n",
      "        [ 56],\n",
      "        [ 57],\n",
      "        [ 58],\n",
      "        [ 59],\n",
      "        [ 60],\n",
      "        [ 61],\n",
      "        [ 62],\n",
      "        [ 63],\n",
      "        [ 64],\n",
      "        [ 65],\n",
      "        [ 66],\n",
      "        [ 67],\n",
      "        [ 68],\n",
      "        [ 69],\n",
      "        [ 70],\n",
      "        [ 71],\n",
      "        [ 72],\n",
      "        [ 73],\n",
      "        [ 74],\n",
      "        [ 75],\n",
      "        [ 76],\n",
      "        [ 77],\n",
      "        [ 78],\n",
      "        [ 79],\n",
      "        [ 80],\n",
      "        [ 81],\n",
      "        [ 82],\n",
      "        [ 83],\n",
      "        [ 84],\n",
      "        [ 85],\n",
      "        [ 86],\n",
      "        [ 87],\n",
      "        [ 88],\n",
      "        [ 89],\n",
      "        [ 90],\n",
      "        [ 91],\n",
      "        [ 92],\n",
      "        [ 93],\n",
      "        [ 94],\n",
      "        [ 95],\n",
      "        [ 96],\n",
      "        [ 97],\n",
      "        [ 98],\n",
      "        [ 99],\n",
      "        [100],\n",
      "        [101],\n",
      "        [102],\n",
      "        [103],\n",
      "        [104],\n",
      "        [105],\n",
      "        [106],\n",
      "        [107],\n",
      "        [108],\n",
      "        [109],\n",
      "        [110],\n",
      "        [111],\n",
      "        [112],\n",
      "        [113],\n",
      "        [114],\n",
      "        [115],\n",
      "        [116],\n",
      "        [117],\n",
      "        [118],\n",
      "        [119],\n",
      "        [120],\n",
      "        [121],\n",
      "        [122],\n",
      "        [123],\n",
      "        [124],\n",
      "        [125],\n",
      "        [126],\n",
      "        [127],\n",
      "        [128],\n",
      "        [129],\n",
      "        [130],\n",
      "        [131],\n",
      "        [132],\n",
      "        [133],\n",
      "        [134],\n",
      "        [135],\n",
      "        [136],\n",
      "        [137],\n",
      "        [138],\n",
      "        [139],\n",
      "        [140],\n",
      "        [141],\n",
      "        [142],\n",
      "        [143],\n",
      "        [144],\n",
      "        [145],\n",
      "        [146],\n",
      "        [147],\n",
      "        [148],\n",
      "        [149],\n",
      "        [150],\n",
      "        [151],\n",
      "        [152],\n",
      "        [153],\n",
      "        [154],\n",
      "        [155],\n",
      "        [156],\n",
      "        [157],\n",
      "        [158],\n",
      "        [159],\n",
      "        [160],\n",
      "        [161],\n",
      "        [162],\n",
      "        [163],\n",
      "        [164],\n",
      "        [165],\n",
      "        [166],\n",
      "        [167],\n",
      "        [168],\n",
      "        [169],\n",
      "        [170],\n",
      "        [171],\n",
      "        [172],\n",
      "        [173],\n",
      "        [174],\n",
      "        [175],\n",
      "        [176],\n",
      "        [177],\n",
      "        [178],\n",
      "        [179],\n",
      "        [180],\n",
      "        [181],\n",
      "        [182],\n",
      "        [183],\n",
      "        [184],\n",
      "        [185],\n",
      "        [186],\n",
      "        [187],\n",
      "        [188],\n",
      "        [189],\n",
      "        [190],\n",
      "        [191],\n",
      "        [192],\n",
      "        [193],\n",
      "        [194],\n",
      "        [195],\n",
      "        [196],\n",
      "        [197],\n",
      "        [198],\n",
      "        [199],\n",
      "        [200],\n",
      "        [201],\n",
      "        [202],\n",
      "        [203],\n",
      "        [204],\n",
      "        [205],\n",
      "        [206],\n",
      "        [207],\n",
      "        [208],\n",
      "        [209],\n",
      "        [210],\n",
      "        [211],\n",
      "        [212],\n",
      "        [213],\n",
      "        [214],\n",
      "        [215],\n",
      "        [216],\n",
      "        [217],\n",
      "        [218],\n",
      "        [219],\n",
      "        [220],\n",
      "        [221],\n",
      "        [222],\n",
      "        [223],\n",
      "        [224],\n",
      "        [225],\n",
      "        [226],\n",
      "        [227],\n",
      "        [228],\n",
      "        [229],\n",
      "        [230],\n",
      "        [231],\n",
      "        [232],\n",
      "        [233],\n",
      "        [234],\n",
      "        [235],\n",
      "        [236],\n",
      "        [237],\n",
      "        [238],\n",
      "        [239],\n",
      "        [240],\n",
      "        [241],\n",
      "        [242],\n",
      "        [243],\n",
      "        [244],\n",
      "        [245],\n",
      "        [246],\n",
      "        [247],\n",
      "        [248],\n",
      "        [249],\n",
      "        [250],\n",
      "        [251],\n",
      "        [252],\n",
      "        [253],\n",
      "        [254],\n",
      "        [255],\n",
      "        [256],\n",
      "        [257],\n",
      "        [258],\n",
      "        [259],\n",
      "        [260],\n",
      "        [261],\n",
      "        [262],\n",
      "        [263],\n",
      "        [264],\n",
      "        [265],\n",
      "        [266],\n",
      "        [267],\n",
      "        [268],\n",
      "        [269],\n",
      "        [270],\n",
      "        [271],\n",
      "        [272],\n",
      "        [273],\n",
      "        [274],\n",
      "        [275],\n",
      "        [276],\n",
      "        [277],\n",
      "        [278],\n",
      "        [279],\n",
      "        [280],\n",
      "        [281],\n",
      "        [282],\n",
      "        [283],\n",
      "        [284],\n",
      "        [285],\n",
      "        [286],\n",
      "        [287],\n",
      "        [288],\n",
      "        [289],\n",
      "        [290],\n",
      "        [291],\n",
      "        [292],\n",
      "        [293],\n",
      "        [294],\n",
      "        [295],\n",
      "        [296],\n",
      "        [297],\n",
      "        [298],\n",
      "        [299]])\n",
      "even_PE: tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 8.4147e-01,  8.2186e-01,  8.0196e-01,  ...,  1.1140e-04,\n",
      "          1.0746e-04,  1.0366e-04],\n",
      "        [ 9.0930e-01,  9.3641e-01,  9.5814e-01,  ...,  2.2279e-04,\n",
      "          2.1492e-04,  2.0733e-04],\n",
      "        ...,\n",
      "        [ 9.9287e-01, -5.8070e-01, -8.0185e-02,  ...,  3.3079e-02,\n",
      "          3.1910e-02,  3.0783e-02],\n",
      "        [ 4.3614e-01, -9.9991e-01,  7.5148e-01,  ...,  3.3190e-02,\n",
      "          3.2018e-02,  3.0887e-02],\n",
      "        [-5.2158e-01, -5.5859e-01,  9.7801e-01,  ...,  3.3302e-02,\n",
      "          3.2125e-02,  3.0990e-02]])\n",
      "odd_PE: tensor([[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        [ 0.5403,  0.5697,  0.5974,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        [-0.4161, -0.3509, -0.2863,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        ...,\n",
      "        [-0.1192, -0.8141,  0.9968,  ...,  0.9995,  0.9995,  0.9995],\n",
      "        [-0.8999,  0.0134,  0.6598,  ...,  0.9994,  0.9995,  0.9995],\n",
      "        [-0.8532,  0.8294, -0.2085,  ...,  0.9994,  0.9995,  0.9995]])\n",
      "stacked: tensor([[[ 0.0000e+00,  1.0000e+00],\n",
      "         [ 0.0000e+00,  1.0000e+00],\n",
      "         [ 0.0000e+00,  1.0000e+00],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  1.0000e+00],\n",
      "         [ 0.0000e+00,  1.0000e+00],\n",
      "         [ 0.0000e+00,  1.0000e+00]],\n",
      "\n",
      "        [[ 8.4147e-01,  5.4030e-01],\n",
      "         [ 8.2186e-01,  5.6969e-01],\n",
      "         [ 8.0196e-01,  5.9738e-01],\n",
      "         ...,\n",
      "         [ 1.1140e-04,  1.0000e+00],\n",
      "         [ 1.0746e-04,  1.0000e+00],\n",
      "         [ 1.0366e-04,  1.0000e+00]],\n",
      "\n",
      "        [[ 9.0930e-01, -4.1615e-01],\n",
      "         [ 9.3641e-01, -3.5090e-01],\n",
      "         [ 9.5814e-01, -2.8629e-01],\n",
      "         ...,\n",
      "         [ 2.2279e-04,  1.0000e+00],\n",
      "         [ 2.1492e-04,  1.0000e+00],\n",
      "         [ 2.0733e-04,  1.0000e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 9.9287e-01, -1.1921e-01],\n",
      "         [-5.8070e-01, -8.1412e-01],\n",
      "         [-8.0185e-02,  9.9678e-01],\n",
      "         ...,\n",
      "         [ 3.3079e-02,  9.9945e-01],\n",
      "         [ 3.1910e-02,  9.9949e-01],\n",
      "         [ 3.0783e-02,  9.9953e-01]],\n",
      "\n",
      "        [[ 4.3614e-01, -8.9988e-01],\n",
      "         [-9.9991e-01,  1.3449e-02],\n",
      "         [ 7.5148e-01,  6.5976e-01],\n",
      "         ...,\n",
      "         [ 3.3190e-02,  9.9945e-01],\n",
      "         [ 3.2018e-02,  9.9949e-01],\n",
      "         [ 3.0887e-02,  9.9952e-01]],\n",
      "\n",
      "        [[-5.2158e-01, -8.5320e-01],\n",
      "         [-5.5859e-01,  8.2944e-01],\n",
      "         [ 9.7801e-01, -2.0854e-01],\n",
      "         ...,\n",
      "         [ 3.3302e-02,  9.9945e-01],\n",
      "         [ 3.2125e-02,  9.9948e-01],\n",
      "         [ 3.0990e-02,  9.9952e-01]]])\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "max_sequence_length = 300\n",
    "\n",
    "\n",
    "absolute_pe = PositionalEncoding(d_model, max_sequence_length)\n",
    "\n",
    "\n",
    "batch_size = 64  \n",
    "positions = torch.arange(max_sequence_length).unsqueeze(0).expand(batch_size, -1)\n",
    "positional_encodings = absolute_pe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "072d9764-20fa-46c2-8929-34918919ec9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "          0.0000e+00,  1.0000e+00],\n",
       "        [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
       "          1.0366e-04,  1.0000e+00],\n",
       "        [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
       "          2.0733e-04,  1.0000e+00],\n",
       "        ...,\n",
       "        [ 9.9287e-01, -1.1921e-01, -5.8070e-01,  ...,  9.9949e-01,\n",
       "          3.0783e-02,  9.9953e-01],\n",
       "        [ 4.3614e-01, -8.9988e-01, -9.9991e-01,  ...,  9.9949e-01,\n",
       "          3.0887e-02,  9.9952e-01],\n",
       "        [-5.2158e-01, -8.5320e-01, -5.5859e-01,  ...,  9.9948e-01,\n",
       "          3.0990e-02,  9.9952e-01]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1dc470-23c9-4e23-a565-ad4431b4c7bb",
   "metadata": {},
   "source": [
    "# RELATIVE POSITIONAL ENCODING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10729f4-6ca8-4177-bb15-b85169f90ac2",
   "metadata": {},
   "source": [
    "## In relative positional encoding each tokens get as many positional embedding as there are tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3084d200-9100-4294-9434-b179d579fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5RelativePositionalEncoder(nn.Module):\n",
    "    def __init__(self, num_heads, max_position=512):\n",
    "        super(T5RelativePositionalEncoder, self).__init__()\n",
    "        self.max_position = max_position\n",
    "        self.embeddings_table = nn.Embedding(max_position*max_position, num_heads)\n",
    "\n",
    "    def forward(self, seq_len_q, seq_len_k):\n",
    "        range_vec_q = torch.arange(seq_len_q)\n",
    "        print(\"range_vec_q :\",range_vec_q)\n",
    "        range_vec_k = torch.arange(seq_len_k)\n",
    "        print(\"range_vec_k :\",range_vec_k)\n",
    "        relative_position = range_vec_k[None, :] - range_vec_q[:, None]\n",
    "        print(\"relative_position :\",relative_position)\n",
    "        relative_position_clipped = torch.clamp(relative_position, -self.max_position, self.max_position)\n",
    "        print(\"relative_position_clipped :\",relative_position_clipped)\n",
    "        final_mat = relative_position_clipped + self.max_position\n",
    "        print(\"final_mat :\",final_mat)\n",
    "        embeddings = self.embeddings_table(final_mat)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "869bf1bd-bad1-417b-b1b2-0baacefc7e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range_vec_q : tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "range_vec_k : tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\n",
      "relative_position : tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14],\n",
      "        [-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
      "        [-2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [-3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
      "        [-4, -3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
      "        [-5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
      "        [-6, -5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
      "        [-7, -6, -5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7],\n",
      "        [-8, -7, -6, -5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5,  6],\n",
      "        [-9, -8, -7, -6, -5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5]])\n",
      "relative_position_clipped : tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14],\n",
      "        [-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
      "        [-2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [-3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
      "        [-4, -3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
      "        [-5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
      "        [-6, -5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
      "        [-7, -6, -5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7],\n",
      "        [-8, -7, -6, -5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5,  6],\n",
      "        [-9, -8, -7, -6, -5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5]])\n",
      "final_mat : tensor([[300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313,\n",
      "         314],\n",
      "        [299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312,\n",
      "         313],\n",
      "        [298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "         312],\n",
      "        [297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310,\n",
      "         311],\n",
      "        [296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n",
      "         310],\n",
      "        [295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308,\n",
      "         309],\n",
      "        [294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "         308],\n",
      "        [293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306,\n",
      "         307],\n",
      "        [292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305,\n",
      "         306],\n",
      "        [291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304,\n",
      "         305]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d_model = 512\n",
    "max_sequence_length = 300\n",
    "\n",
    "\n",
    "absolute_re = T5RelativePositionalEncoder(d_model, max_sequence_length)\n",
    "\n",
    "seq_len_q = 10\n",
    "seq_len_k = 15\n",
    "batch_size = 64  \n",
    "positions = torch.arange(max_sequence_length).unsqueeze(0).expand(batch_size, -1)\n",
    "relative_encodings = absolute_re(seq_len_q,seq_len_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4ac48ece-32c1-4783-adc2-2480fc72da99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.0505e-01,  9.9041e-01, -5.2244e-01,  ..., -1.4293e-01,\n",
       "          -1.6684e+00, -1.3944e+00],\n",
       "         [ 3.7672e-02, -3.0596e-01, -2.4705e+00,  ..., -6.5935e-01,\n",
       "           6.7230e-01, -1.9949e-01],\n",
       "         [-5.3505e-01, -2.9749e-01,  9.6660e-02,  ..., -8.9415e-01,\n",
       "          -3.7038e-01,  4.6801e-01],\n",
       "         ...,\n",
       "         [ 1.4012e+00,  3.2748e-02,  9.2254e-01,  ...,  2.7417e+00,\n",
       "           1.4758e+00,  9.2321e-01],\n",
       "         [ 1.2565e+00, -1.0146e+00,  7.4777e-01,  ...,  1.0888e+00,\n",
       "           8.8507e-01,  3.6969e-01],\n",
       "         [ 1.6324e+00,  1.3988e+00, -1.3409e+00,  ..., -3.8702e-01,\n",
       "           1.2695e+00, -7.6887e-02]],\n",
       "\n",
       "        [[-2.7509e-01, -1.0364e-02, -6.5098e-01,  ...,  6.7114e-01,\n",
       "          -8.9518e-01, -4.6780e-01],\n",
       "         [-2.0505e-01,  9.9041e-01, -5.2244e-01,  ..., -1.4293e-01,\n",
       "          -1.6684e+00, -1.3944e+00],\n",
       "         [ 3.7672e-02, -3.0596e-01, -2.4705e+00,  ..., -6.5935e-01,\n",
       "           6.7230e-01, -1.9949e-01],\n",
       "         ...,\n",
       "         [ 2.8737e-02, -2.8606e-01, -4.2060e-01,  ..., -4.7439e-02,\n",
       "           3.8527e-01,  1.9454e+00],\n",
       "         [ 1.4012e+00,  3.2748e-02,  9.2254e-01,  ...,  2.7417e+00,\n",
       "           1.4758e+00,  9.2321e-01],\n",
       "         [ 1.2565e+00, -1.0146e+00,  7.4777e-01,  ...,  1.0888e+00,\n",
       "           8.8507e-01,  3.6969e-01]],\n",
       "\n",
       "        [[-1.9816e+00,  1.4070e-01,  5.9226e-01,  ..., -1.6686e+00,\n",
       "          -1.3356e+00,  1.3681e+00],\n",
       "         [-2.7509e-01, -1.0364e-02, -6.5098e-01,  ...,  6.7114e-01,\n",
       "          -8.9518e-01, -4.6780e-01],\n",
       "         [-2.0505e-01,  9.9041e-01, -5.2244e-01,  ..., -1.4293e-01,\n",
       "          -1.6684e+00, -1.3944e+00],\n",
       "         ...,\n",
       "         [ 7.4791e-01, -1.8680e-02, -1.5855e+00,  ...,  3.7200e-01,\n",
       "          -1.0479e+00,  1.4216e+00],\n",
       "         [ 2.8737e-02, -2.8606e-01, -4.2060e-01,  ..., -4.7439e-02,\n",
       "           3.8527e-01,  1.9454e+00],\n",
       "         [ 1.4012e+00,  3.2748e-02,  9.2254e-01,  ...,  2.7417e+00,\n",
       "           1.4758e+00,  9.2321e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-7.1082e-01,  2.4115e+00,  3.2497e-02,  ..., -1.3708e+00,\n",
       "           8.9050e-01,  6.6228e-01],\n",
       "         [-1.9000e-01,  1.2532e+00, -8.4785e-03,  ...,  1.8954e+00,\n",
       "          -1.2475e+00,  1.3056e+00],\n",
       "         [ 4.8364e-01, -9.1310e-01, -9.5194e-01,  ..., -8.7106e-01,\n",
       "           1.3627e+00, -1.1616e-01],\n",
       "         ...,\n",
       "         [ 1.6279e+00, -6.8178e-01,  5.8855e-01,  ..., -1.3714e+00,\n",
       "          -5.0750e-01, -3.9689e-01],\n",
       "         [ 1.5684e+00, -1.6306e-01,  6.7388e-01,  ..., -1.5633e+00,\n",
       "          -5.9721e-01,  2.4148e+00],\n",
       "         [ 2.7461e-01, -1.7089e+00, -9.4864e-02,  ...,  5.5257e-01,\n",
       "          -6.7025e-01, -1.1219e-01]],\n",
       "\n",
       "        [[-8.9617e-01, -1.7177e+00, -5.7479e-01,  ...,  5.6763e-01,\n",
       "           8.1237e-01, -5.2433e-01],\n",
       "         [-7.1082e-01,  2.4115e+00,  3.2497e-02,  ..., -1.3708e+00,\n",
       "           8.9050e-01,  6.6228e-01],\n",
       "         [-1.9000e-01,  1.2532e+00, -8.4785e-03,  ...,  1.8954e+00,\n",
       "          -1.2475e+00,  1.3056e+00],\n",
       "         ...,\n",
       "         [-1.2000e+00, -2.0420e+00, -7.2347e-01,  ...,  1.6855e+00,\n",
       "          -8.4043e-01,  1.6520e+00],\n",
       "         [ 1.6279e+00, -6.8178e-01,  5.8855e-01,  ..., -1.3714e+00,\n",
       "          -5.0750e-01, -3.9689e-01],\n",
       "         [ 1.5684e+00, -1.6306e-01,  6.7388e-01,  ..., -1.5633e+00,\n",
       "          -5.9721e-01,  2.4148e+00]],\n",
       "\n",
       "        [[ 5.2952e-01, -6.5566e-01, -2.6522e-01,  ..., -1.1088e-01,\n",
       "          -1.5885e-03,  1.1046e+00],\n",
       "         [-8.9617e-01, -1.7177e+00, -5.7479e-01,  ...,  5.6763e-01,\n",
       "           8.1237e-01, -5.2433e-01],\n",
       "         [-7.1082e-01,  2.4115e+00,  3.2497e-02,  ..., -1.3708e+00,\n",
       "           8.9050e-01,  6.6228e-01],\n",
       "         ...,\n",
       "         [-8.6593e-02,  6.2993e-01,  1.3181e+00,  ..., -7.9458e-01,\n",
       "          -3.5297e-01,  2.8825e-01],\n",
       "         [-1.2000e+00, -2.0420e+00, -7.2347e-01,  ...,  1.6855e+00,\n",
       "          -8.4043e-01,  1.6520e+00],\n",
       "         [ 1.6279e+00, -6.8178e-01,  5.8855e-01,  ..., -1.3714e+00,\n",
       "          -5.0750e-01, -3.9689e-01]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0c4e1f-eb5d-41da-8435-d6c1129d66f8",
   "metadata": {},
   "source": [
    "### But why use relative positional encoding ?\n",
    "\n",
    "- Relative positional encoding can be beneficial in scenarios where the model needs to capture dependencies between tokens that are based on their relative positions rather than their absolute positions. This can be especially useful in tasks where the order of tokens in a sequence is important but not necessarily fixed, such as language modeling or time series forecasting.\r\n",
    "- \r\n",
    "The skewing operation in Srel is designed to model these relative dependencies efficiently. By introducing relative positional embeddings, the mode l can learn to attend to tokens that are at different distances from the current token, which can help capture long-range dependencies\n",
    ".\r\n",
    "\r\n",
    "In contrast, absolute positional encoding provides a fixed embedding for each position in the sequence, but it doesn't inherently capture relative positional information. The use of relative positional encoding can potentially improve the model's performance on tasks where relative positions matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cd5030-4be2-4ed9-ac09-3c2c3066ef91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
